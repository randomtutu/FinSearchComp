"""Utilities for retrieving and storing market snapshot data via akshare."""

from __future__ import annotations

import datetime as dt
import glob
import logging
import os
import re
import time
from typing import Any, Callable, Dict, Optional

import akshare as ak
import pandas as pd

logger = logging.getLogger(__name__)

# Simple in-process cache to avoid refetching the same snapshot repeatedly.
_CACHE: Dict[str, Dict[str, object]] = {}
_SNAPSHOT_CACHE: Dict[str, Dict[str, Any]] = {}

PACKAGE_ROOT = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(PACKAGE_ROOT))
DEFAULT_SNAPSHOT_DIR = os.path.join(PROJECT_ROOT, "data", "market_snapshots")
SNAPSHOT_DIR_ENV = "FINSEARCHCOMP_SNAPSHOT_DIR"
_SNAPSHOT_DIR = os.environ.get(SNAPSHOT_DIR_ENV, DEFAULT_SNAPSHOT_DIR)

UNIVERSE: Dict[str, Callable[[], pd.DataFrame]] = {
    "CN_STOCK": lambda: ak.stock_zh_a_spot_em(),  # A股全量快照
    "US_STOCK": lambda: ak.stock_us_spot_em(),  # 美股全量快照
    "HK_STOCK": lambda: ak.stock_hk_spot_em(),  # 港股全量快照
    "CN_INDEX": lambda: ak.stock_zh_index_spot_em("沪深重要指数"),  # 指数全量快照
    "H_INDEX": lambda: ak.stock_hk_index_spot_em(),  # hk index
    "GLOBAL_IDX": lambda: ak.index_global_spot_em(),  # 全球指数全量快照
    "FX": lambda: ak.forex_spot_em(),  # 汇率全量快照
}


def set_snapshot_directory(path: str) -> None:
    """Override the snapshot directory used when loading cached market data."""
    global _SNAPSHOT_DIR
    _SNAPSHOT_DIR = path


def get_snapshot_directory() -> str:
    """Return the directory where pre-fetched market snapshots are stored."""
    return _SNAPSHOT_DIR


def _resolve_snapshot_path(universe: str) -> str:
    directory = get_snapshot_directory()
    if not os.path.isdir(directory):
        raise FileNotFoundError(f"Snapshot directory does not exist: {directory}")

    base = universe.lower()
    candidates = []
    for ext in ("csv", "json"):
        pattern = os.path.join(directory, f"{base}*.{ext}")
        candidates.extend(glob.glob(pattern))

    if not candidates:
        raise FileNotFoundError(f"No snapshot files found for {universe} in {directory}")

    latest = max(candidates, key=os.path.getmtime)
    return latest


def _load_snapshot_df(universe: str) -> pd.DataFrame:
    path = _resolve_snapshot_path(universe)
    mtime = os.path.getmtime(path)
    entry = _SNAPSHOT_CACHE.get(universe)
    if entry and entry.get("path") == path and entry.get("mtime") == mtime:
        return entry["df"]

    if path.endswith(".csv"):
        df = pd.read_csv(path)
    else:
        df = pd.read_json(path, orient="records")

    _SNAPSHOT_CACHE[universe] = {"path": path, "mtime": mtime, "df": df}
    logger.debug("Loaded snapshot for %s from %s", universe, path)
    return df


def _get_df(universe: str, ttl_sec: int = 60, force_refresh: bool = False) -> pd.DataFrame:
    """Return a cached snapshot for the given universe, refreshing when expired."""
    now = time.time()
    entry = _CACHE.get(universe)
    if force_refresh or not entry or now - entry["ts"] > ttl_sec:
        logger.debug("Refreshing market snapshot for %s from AkShare", universe)
        df = UNIVERSE[universe]()
        _CACHE[universe] = {"ts": now, "df": df}
    return _CACHE[universe]["df"]  # type: ignore[return-value]


def _norm_cn(code: str) -> str:
    cleaned = code.strip().upper()
    return cleaned.split(".")[0] if cleaned.endswith((".SH", ".SZ", ".BJ")) else cleaned


def _norm_os(code: str) -> tuple[str, str]:
    raw = code.strip().upper()
    if raw.endswith(".USA"):
        return "US_STOCK", raw[:-4]
    if raw.endswith(".US"):
        return "US_STOCK", raw[:-3]
    if raw.endswith(".HK"):
        return "HK_STOCK", raw[:-3].zfill(5)
    if re.fullmatch(r"\d{1,5}", raw):
        return "HK_STOCK", raw.zfill(5)
    return "US_STOCK", raw  # 兼容 105.AAPL 或 AAPL


def _pick_row(df: pd.DataFrame, code: str, name_hint: Optional[str] = None) -> Optional[pd.Series]:
    cols = set(df.columns.astype(str))
    if "代码" in cols:
        s = df["代码"].astype(str).str.upper()
        hit = df[s == code.upper()]
        if hit.empty and "." in code:
            hit = df[s.str.endswith("." + code.split(".")[-1].upper())]
        if not hit.empty:
            return hit.iloc[0]
    if "名称" in cols:
        key = (name_hint or code).upper()
        hit = df[df["名称"].astype(str).str.upper().str.contains(key, na=False)]
        if not hit.empty:
            return hit.iloc[0]
    if "代码" in cols:
        hit = df[df["代码"].astype(str).str.upper().str.contains(code.upper(), na=False)]
        if not hit.empty:
            return hit.iloc[0]
    return None


def _as_quote(row: pd.Series, orig_ticker: str) -> Dict[str, object]:
    getter = row.get

    def _get_float(primary: str, alt: Optional[str] = None) -> Optional[float]:
        value = getter(primary, getter(alt, None))
        return float(value) if value not in (None, "") else None

    fetch_dt_utc = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)
    return {
        "ticker": orig_ticker,
        "name": getter("名称", None),
        "price": _get_float("最新价", "当前价"),
        "change": _get_float("涨跌额"),
        "pct_change": _get_float("涨跌幅"),
        "open": _get_float("今开", "开盘价"),
        "high": _get_float("最高", "最高价"),
        "low": _get_float("最低", "最低价"),
        "prev_close": _get_float("昨收", "昨收价"),
        "volume": _get_float("成交量"),
        "amount": _get_float("成交额"),
        "quote_time": getter("最新行情时间", getter("时间", None)),
        "fetch_time_utc": fetch_dt_utc.isoformat(),
        "fetch_time_epoch": int(fetch_dt_utc.timestamp()),
    }


def fetch_single(ticker: str, tags: str, method: Optional[str], ttl_sec: int = 60) -> Optional[Dict[str, object]]:
    """
    Retrieve a single quote snapshot by routing to the appropriate market universe.

    :param ticker: The identifier provided by akshare prompt metadata.
    :param tags: Tag string, e.g. {"汇率","指数","股票","Index","Stock"}.
    :param method: Additional hint used for certain index lookups.
    :param ttl_sec: Cache TTL for each universe snapshot.
    """
    tag = tags.strip()
    if tag == "汇率":
        universe, code, name_hint = "FX", ticker.strip().upper(), None
    elif tag == "指数" and method == "akshare_HK_index":
        universe, code, name_hint = "H_INDEX", ticker, None
    elif tag == "指数" and method == "akshare_沪深重要指数_index":
        universe, code, name_hint = "CN_INDEX", _norm_cn(ticker), None
    elif tag == "Index":
        universe = "GLOBAL_IDX"
        code = ticker.strip().upper()
        name_hint = code
    elif tag == "股票":
        universe, code, name_hint = "CN_STOCK", _norm_cn(ticker), None
    elif tag == "Stock":
        universe, code = _norm_os(ticker)
        name_hint = None
    else:
        raise ValueError(f"未知 tags: {tags}")

    try:
        df = _load_snapshot_df(universe)
    except FileNotFoundError as err:
        logger.error("Snapshot missing for %s: %s", universe, err)
        return None
    except Exception as exc:  # pragma: no cover - defensive logging for IO issues
        logger.exception("Failed to load snapshot for %s: %s", universe, exc)
        return None

    if df is None or df.empty:
        logger.warning("Snapshot for %s is empty", universe)
        return None

    row = _pick_row(df, code, name_hint)
    return _as_quote(row, ticker) if row is not None else None


def fetch_all_universe_data(ttl_sec: int = 60, force_refresh: bool = False) -> Dict[str, pd.DataFrame]:
    """Return DataFrames for all configured universes."""
    snapshots: Dict[str, pd.DataFrame] = {}
    for universe in UNIVERSE:
        try:
            snapshots[universe] = _get_df(universe, ttl_sec=ttl_sec, force_refresh=force_refresh)
        except Exception as exc:  # pragma: no cover - network code
            logger.exception("Failed to fetch universe %s: %s", universe, exc)
            raise
    return snapshots


def save_universe_dataframes(
    output_dir: str,
    ttl_sec: int = 60,
    force_refresh: bool = False,
    file_format: str = "csv",
    timestamped: bool = True,
) -> Dict[str, str]:
    """
    Fetch all universes and persist them to disk.

    :returns: Mapping of universe name to saved file path.
    """
    os.makedirs(output_dir, exist_ok=True)
    snapshots = fetch_all_universe_data(ttl_sec=ttl_sec, force_refresh=force_refresh)

    fmt = file_format.lower()
    if fmt not in {"csv", "json"}:
        raise ValueError(f"Unsupported file format: {file_format}")

    saved_paths: Dict[str, str] = {}
    timestamp = dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ") if timestamped else ""

    for universe, df in snapshots.items():
        suffix = f"_{timestamp}" if timestamp else ""
        filename = f"{universe.lower()}{suffix}.{fmt}"
        path = os.path.join(output_dir, filename)
        if fmt == "csv":
            df.to_csv(path, index=False, encoding="utf-8")
        else:
            df.to_json(path, orient="records", force_ascii=False)
        saved_paths[universe] = path
        logger.info("Saved %s snapshot to %s", universe, path)
    return saved_paths
